# Video Processing with RunPod - Serverless & Pod Options

Intelligent video processing system for Google Meet recordings, screen shares, and app demonstrations using Qwen3-VL-8B (October 2025).

## ğŸ¯ Two Deployment Options

### Option 1: Pod (Recommended) â­
Persistent GPU instance with adaptive scaling - **best for screen recordings and long videos**.

- âœ… **Latest Model**: Qwen3-VL-8B-Instruct (Oct 15, 2025) with 32-language OCR
- âœ… **Auto-Scaling**: Adapts to 24GB / 48GB / 80GB VRAM automatically
- âœ… **Fast**: 1-hour video in 15 minutes (24GB) or 6 minutes (80GB)
- âœ… **Cost-Effective**: $0.06 per video (Spot pricing)
- âœ… **Screen Understanding**: Reads UI text, detects elements, understands context

**Perfect for**: Google Meet recordings, app demos, screen shares, long videos (>10 min)

[ğŸ“– Pod Documentation](pod/README.md) | [âš–ï¸ Pod vs Serverless Comparison](POD_VS_SERVERLESS.md)

### Option 2: Serverless
Auto-scaling functions for multiple models - **best for short clips and multi-model workflows**.

- âš ï¸ **YOLO Limitation**: Cannot analyze screen content (detects generic objects only)
- âœ… **Multiple Models**: Can run YOLO + Whisper + other models independently
- âœ… **No Management**: Auto-scales from 0 to N workers
- âš ï¸ **Slower**: 35 minutes for 1-hour video
- âš ï¸ **More Expensive**: ~10Ã— cost vs Pod for long videos

**Perfect for**: Short videos (<5 min), sporadic processing, multi-model workflows

[ğŸ“– Serverless Documentation](#serverless-documentation-legacy)

---

## ğŸš€ Quick Start: Deploy Pod (5 minutes)

### Step 1: Build Docker Image

```bash
cd pod
chmod +x deploy.sh
./deploy.sh your-docker-username
```

### Step 2: Deploy to RunPod

1. Go to [RunPod Pods](https://www.runpod.io/console/pods)
2. Click "Deploy"
3. Select **RTX 4090 Spot** (24GB, $0.25/hour)
4. Container Settings:
   - Docker Image: `your-docker-username/video-processor:latest`
   - Expose HTTP Ports: `8000`
   - Container Disk: `50GB`
5. Deploy and copy your pod URL

### Step 3: Process Your First Video

```bash
# Install client dependencies
pip install requests

# Process video
python scripts/process_video_pod.py \
    "your-video.mp4" \
    --pod-url "https://your-pod-id-8000.proxy.runpod.net" \
    --mode screen_share \
    --output results.json
```

**That's it!** Your video will be analyzed with:
- Full screen content understanding
- UI element detection with positions
- Text extraction (OCR)
- Scene descriptions and context
- Activity and feature identification

[ğŸ“– Detailed Pod Documentation](pod/README.md)

---

## ğŸ“Š Cost & Performance Comparison

**1-hour Google Meet recording with app demo:**

| Method | Time | Cost | Screen Analysis |
|--------|------|------|----------------|
| **Pod (RTX 4090 Spot)** | 15 min | **$0.06** | âœ… Full understanding |
| **Pod (A100 80GB Spot)** | 6 min | **$0.15** | âœ… Full understanding |
| Serverless YOLO | 35 min | $0.63 | âŒ Useless for screens |

**Winner**: Pod is 2-3Ã— faster and 10Ã— cheaper, and actually understands screen content!

[âš–ï¸ See detailed comparison](POD_VS_SERVERLESS.md)

---

## ğŸ“š Documentation

### Pod (Recommended)
- [ğŸ“– Pod Setup & Usage Guide](pod/README.md) - Complete pod documentation
- [âš–ï¸ Pod vs Serverless](POD_VS_SERVERLESS.md) - Which to use and why
- [ğŸ¯ VLM vs YOLO](VLM_VS_YOLO.md) - Why YOLO fails for screen recordings

### Serverless (Legacy)
- [ğŸ“– Serverless Documentation](#serverless-documentation-legacy) - Original serverless setup
- [ğŸ“– GitHub Actions Setup](GITHUB_ACTIONS_SETUP.md) - Automated Docker builds
- [ğŸ“– RunPod Endpoint Deployment](RUNPOD_ENDPOINT_DEPLOYMENT_GUIDE.md) - Serverless deployment

### General
- [ğŸ“‹ Implementation Plan](IMPLEMENTATION_PLAN.md) - Complete project roadmap
- [ğŸ“„ PRD](PRD.md) - Product requirements
- [ğŸ“Š Progress Tracker](PROGRESS.md) - Current status

---

<a name="serverless-documentation-legacy"></a>
## ğŸ“¦ Serverless Documentation (Legacy)

> **Note**: The serverless approach is not recommended for Google Meet recordings or screen shares.
> YOLO cannot analyze screen content - it only detects generic objects (person, laptop, tv).
> See [VLM_VS_YOLO.md](VLM_VS_YOLO.md) for details.

## ğŸ¯ Project Status

**Current Phase**: Phase 1 Complete âœ“

- âœ… Phase 0: 40% (Automated tasks complete, manual RunPod setup pending)
- âœ… Phase 1: 100% Complete
- â³ Phase 2-12: Not started

See [PROGRESS.md](PROGRESS.md) for detailed status.

---

## ğŸš€ Quick Start

### Prerequisites Completed âœ“

- âœ… Python 3.13.3 installed
- âœ… Docker 28.3.0 installed
- âœ… Git 2.49.0 installed
- âœ… Project structure created
- âœ… Virtual environment configured
- âœ… Dependencies installed

### Next Steps (You Need To Do)

1. **Create RunPod Account** â³
   - Go to https://www.runpod.io/
   - Sign up and verify email
   - Add credits ($25-50 recommended for testing)

2. **Get API Key** â³
   - Login to RunPod dashboard
   - Settings â†’ API Keys â†’ Create API Key
   - Copy the key

3. **Configure Environment** â³
   ```powershell
   # Edit .env file and add your API key
   notepad .env
   # Replace: RUNPOD_API_KEY=your_runpod_api_key_here
   ```

4. **Verify Setup**
   ```powershell
   .\venv\Scripts\python.exe scripts\test_setup.py
   ```

---

## ğŸ“ Project Structure

```
e:\New folder (3)\runpod\
â”œâ”€â”€ config/              # Configuration files
â”œâ”€â”€ services/            # Service implementations
â”‚   â”œâ”€â”€ ingestion/      # Video/audio ingestion
â”‚   â”œâ”€â”€ detection/      # Object detection (RunPod YOLO client)
â”‚   â”œâ”€â”€ audio/          # Audio processing (RunPod Whisper client)
â”‚   â””â”€â”€ vlm/            # Vision-language model (RunPod VLM client)
â”œâ”€â”€ data/               # Data storage
â”‚   â”œâ”€â”€ videos/         # Input videos
â”‚   â””â”€â”€ frames/         # Extracted frames
â”œâ”€â”€ scripts/            # Utility scripts
â”œâ”€â”€ utils/              # Helper utilities
â”œâ”€â”€ tests/              # Test files
â”œâ”€â”€ venv/               # Python virtual environment
â”œâ”€â”€ .env                # Environment variables (ADD YOUR API KEY HERE!)
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ IMPLEMENTATION_PLAN.md  # Detailed implementation guide
```

---

## ğŸ”§ Architecture

This system uses **RunPod Serverless Endpoints** instead of local GPU inference:

```
LiveKit Stream â†’ Ingestion Service â†’ Redis
                                      â†“
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â†“                     â†“
                 Detection Service      Audio Service
                 (calls RunPod          (calls RunPod
                  YOLO endpoint)        Whisper endpoint)
                           â†“                     â†“
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                              VLM Reasoning Service
                              (calls RunPod VLM endpoint)
                                      â†“
                              PostgreSQL + Events
```

### Why RunPod Serverless?

âœ… **No expensive GPUs needed** - No RTX 4090, A100, etc.
âœ… **Auto-scaling** - Scales from 0 to N workers automatically
âœ… **Pay-per-second** - Only pay when processing
âœ… **Develop anywhere** - Works on any laptop (Mac, Windows, Linux)
âœ… **Cost-effective** - ~$30-50/hour vs $200-400 for dedicated GPU

---

## ğŸ“‹ Implementation Guide

See [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md) for the complete, phase-by-phase implementation guide with 200+ testable tasks.

### Phase Overview

1. **Phase 0-1**: Environment Setup âœ… (mostly complete)
2. **Phase 2**: Database & Storage Setup
3. **Phase 3**: RunPod Endpoint Deployment
4. **Phase 4**: Ingestion Service
5. **Phase 5**: Detection Service (RunPod client)
6. **Phase 6**: Audio Processing Service (RunPod client)
7. **Phase 7**: VLM Reasoning Service (RunPod client)
8. **Phase 8**: Docker Orchestration
9. **Phase 9**: End-to-End Testing
10. **Phase 10-12**: Advanced Features & Production

---

## ğŸ› ï¸ Development

### Activate Virtual Environment

```powershell
# Windows PowerShell
.\venv\Scripts\Activate.ps1

# Or directly run Python
.\venv\Scripts\python.exe
```

### Install Dependencies

```powershell
.\venv\Scripts\pip.exe install -r requirements.txt
```

### Run Tests

```powershell
.\venv\Scripts\python.exe scripts\test_setup.py
```

### Building Docker Images

**Option 1: Automated with GitHub Actions (Recommended)**

Push your code to GitHub and let GitHub Actions build Docker images automatically:

```bash
git add .
git commit -m "Update endpoint code"
git push origin main
```

Images are built 3x faster than local builds and pushed to Docker Hub automatically. See [GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md) for setup instructions.

**Option 2: Manual Local Build**

```bash
# Build YOLO endpoint
docker build -t <your-dockerhub-username>/yolo11-runpod:latest ./endpoints/yolo
docker push <your-dockerhub-username>/yolo11-runpod:latest

# Build Whisper endpoint
docker build -t <your-dockerhub-username>/whisper-runpod:latest ./endpoints/whisper
docker push <your-dockerhub-username>/whisper-runpod:latest
```

---

## ğŸ“š Documentation

- [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md) - Complete implementation guide
- [PRD.md](PRD.md) - Product requirements document
- [PROGRESS.md](PROGRESS.md) - Current progress tracker
- [PHASE_0_1_SETUP_GUIDE.md](PHASE_0_1_SETUP_GUIDE.md) - Setup instructions
- [PHASE_0_1_COMPLETE.md](PHASE_0_1_COMPLETE.md) - Completion summary
- [GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md) - Automated Docker builds with GitHub Actions
- [RUNPOD_ENDPOINT_DEPLOYMENT_GUIDE.md](RUNPOD_ENDPOINT_DEPLOYMENT_GUIDE.md) - RunPod deployment guide
- [ENDPOINT_RESEARCH_SUMMARY.md](ENDPOINT_RESEARCH_SUMMARY.md) - Endpoint research findings

---

## ğŸ”‘ Environment Variables

Required in `.env` file:

```bash
# RunPod (Required for Phase 3+)
RUNPOD_API_KEY=your_key_here              # âš ï¸ REQUIRED
RUNPOD_YOLO_ENDPOINT_ID=                   # From Phase 3
RUNPOD_WHISPER_ENDPOINT_ID=                # From Phase 3
RUNPOD_VLM_ENDPOINT_ID=                    # From Phase 3

# Database (Required for Phase 2+)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=videoagent
POSTGRES_USER=postgres
POSTGRES_PASSWORD=change_me_in_production

# Redis (Required for Phase 2+)
REDIS_HOST=localhost
REDIS_PORT=6379

# Storage (Required for Phase 2+)
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=admin
MINIO_SECRET_KEY=changeme123
```

---

## ğŸ’° Cost Estimation

Based on RunPod serverless pricing:

- **YOLO Detection**: ~$0.0004 per image (RTX 4090)
- **Whisper ASR**: ~$0.001 per minute of audio (A40)
- **VLM Reasoning**: ~$0.003 per inference (A100)

**Example**: Processing 1 hour of 30fps video â‰ˆ $30-50

Compare to dedicated GPU server: $200-400/hour

---

## ğŸ› Troubleshooting

### "ModuleNotFoundError"
```powershell
.\venv\Scripts\pip.exe install -r requirements.txt
```

### "Virtual environment not found"
```powershell
python -m venv venv
.\venv\Scripts\pip.exe install -r requirements.txt
```

### "RunPod API key not set"
Edit `.env` and add your RunPod API key from https://www.runpod.io/console

---

## ğŸ¤ Contributing

This is an implementation of the [PRD.md](PRD.md) specification using RunPod serverless infrastructure.

---

## ğŸ“„ License

[Add your license here]

---

## ğŸ”— Resources

- **RunPod**: https://www.runpod.io/
- **RunPod Docs**: https://docs.runpod.io/
- **RunPod Serverless**: https://docs.runpod.io/serverless/overview

---

**Status**: âœ… Phase 1 Complete | â³ Awaiting RunPod Account Setup | ğŸš€ Ready to Deploy Endpoints

*Last updated: 2025-01-XX*
